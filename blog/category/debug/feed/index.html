<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>debug &#8211; Dev-Pro Informatique</title>
	<atom:link href="http://localhost/wordpress/category/debug/feed/" rel="self" type="application/rss+xml" />
	<link>http://dev-pro.xyz/blog/</link>
	<description>Solution Technologique</description>
	<lastBuildDate>Fri, 27 Dec 2019 15:50:37 +0000</lastBuildDate>
	<language>fr-CA</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.2</generator>
	<item>
		<title>Storing crash data of the Linux kernel for post-crash debugging</title>
		<link>https://dev-pro.xyz/blog/storing-crash-data-of-the-linux-kernel-for-post-crash-debugging/</link>
				<pubDate>Fri, 27 Dec 2019 15:50:37 +0000</pubDate>
		<dc:creator><![CDATA[siz]]></dc:creator>
				<category><![CDATA[crash]]></category>
		<category><![CDATA[crashdump]]></category>
		<category><![CDATA[debug]]></category>
		<category><![CDATA[kernel]]></category>
		<category><![CDATA[kexec]]></category>
		<category><![CDATA[Linux]]></category>
		<category><![CDATA[vmcore]]></category>
		<category><![CDATA[WhitePaper]]></category>

		<guid isPermaLink="false">https://dev-pro.xyz/blog/storing-crash-data-of-the-linux-kernel-for-post-crash-debugging/</guid>
				<description><![CDATA[Logging problems are key features of any complex system in order to detect and locate any unexpected behavior. On Linux system, there are lots of solutions to generate debugging information for an unexpected behavior of a userspace application (log messages,&#160;core dump). But what could we do if there is a kernel problem ? Few solutions ... <a href="https://dev-pro.xyz/blog/storing-crash-data-of-the-linux-kernel-for-post-crash-debugging/" class="more-link text-uppercase small"><strong>Continue Reading</strong> <i class="fa fa-angle-double-right" aria-hidden="true"></i></a>]]></description>
								<content:encoded><![CDATA[<div>
<p>Logging problems are key features of any complex system in order to detect and locate any unexpected behavior. On Linux system, there are lots of solutions to generate debugging information for an unexpected behavior of a userspace application (log messages,&nbsp;<em>core dump</em>).</p>
<p>But what could we do if there is a kernel problem ? Few solutions exist although none are trivial.</p>
<h2 id="what-can-go-wrong-with-a-linux-kernel">What can go wrong with a Linux kernel</h2>
<p>In the first place, one can wonder what are the possible causes of kernel crashes, especially on embedded systems.</p>
<p>Here are several cases where debugging data are critical:</p>
<ol>
<li>Kernel crash due to hardware interrupt: this may be an invalid memory access, any memory-related problem (like&nbsp;DataAbort&nbsp;interrupts on ARM) or any unhandled hardware-related problem.</li>
<li>Kernel crash due to voluntary <strong>panic</strong>: the kernel code detects a problem and may trigger a&nbsp;<em>kernel panic</em>&nbsp;or a&nbsp;<em>kernel oops</em>.</li>
<li>Kernel scheduling problem: some issues with the preemption or with the execution of tasks (userspace or kernel thread).</li>
<li>Kernel deadlock: kernel is stuck due to misuses of kernel locking mechanisms like&nbsp;<em>spinlocks</em>.</li>
<li>Endless raw critical section: code disables IRQ handling and never enables it back.</li>
</ol>
<p>Except for case 5, handling all of these problems means detecting and logging a&nbsp;<em>kernel panic</em>. Generic code exists inside the kernel Linux to detect these cases and trigger a&nbsp;<em>kernel panic</em>&nbsp;when they happen. The default&nbsp;<em>DataAbort</em>&nbsp;handler causes a&nbsp;<em>kernel panic</em>. Detecting kernel deadlock could be done using the&nbsp;<a href="https://www.kernel.org/doc/Documentation/lockup-watchdogs.txt">lockup detector</a>. Also, any non-critical error like&nbsp;<em>kernel oops</em>&nbsp;can be converted into a kernel panic using the kernel&nbsp;<em>sysctl</em>&nbsp;<code>panic_on_oops</code>&nbsp;or with the kernel boot parameter&nbsp;<code>oops=panic</code>. There is also a&nbsp;<code>panic_on_warn</code>&nbsp;parameter to trigger a&nbsp;<em>panic</em>&nbsp;when the kernel executes the&nbsp;<code>WARN()</code>&nbsp;macro.</p>
<p>One must compromise between crashing the kernel on error and the stability of the system. You should consider which is better between triggering a crash or letting the system live after this error.</p>
<p>On embedded systems, rebooting in case of unexpected behavior is often preferred to keeping on with a system which potentially does not fulfill its job.</p>
<p>Rebooting on a kernel crash could be done:</p>
<ol>
<li>In software by setting the&nbsp;<strong>panic timeout</strong>, which is the time between a panic and the effective reboot. Its is defined in the kernel configuration&nbsp;<code>CONFIG_PANIC_TIMEOUT</code> and can also be set from kernel boot parameter.</li>
<li>In hardware using a watchdog. This will happen automatically since, after a crash, the hardware watchdog won&rsquo;t be fed anymore and it will trigger a reboot after its timeout.</li>
</ol>
<p>Ensuring that an embedded system works properly is crucial. Therefore in order to detect the problem on products&nbsp;<em>in the wild</em>&nbsp;and to debug them, all available information on the issue have to be logged persistently.</p>
<p>The first information you may want is the kernel&nbsp;<em>log buffer</em>&nbsp;(aka&nbsp;<code>dmesg</code>). Getting those information will be developed as the main subjet of this article:&nbsp;<strong>How can we log persistently debugging information between a kernel crash and a reboot ?</strong></p>
<h2 id="dumping-the-kernel-log-buffer">Dumping the kernel log buffer</h2>
<p>There is no easy way to write persistently something&nbsp;<em>just after</em>&nbsp;a kernel crash occurs. The main reason is that the kernel can&rsquo;t bet trusted to save the data into disk.</p>
<p>Depending on the physical medium used to dump the data (flash nand/nor, eMMC/SD card, SATA disk, USB disk, &hellip;), the associated&nbsp;<em>subsystem</em>&nbsp;and all the drivers used to perform a dump&nbsp;<strong>must work correctly</strong>, even after a crash, which is impossible to ensure.</p>
<p>Also, when entering in&nbsp;<code>panic()</code>, all CPU are stopped and there is no more scheduling: the kernel stays in the panic function until the machine is rebooted. This means that the necessary code to perform a write on physical medium must be&nbsp;<strong>synchronous</strong>&nbsp;and never depend on scheduler which is not the case for the normal kernel paths.</p>
<p>Imagine that you want to dump kernel log on a eMMC and the kernel crashes while a transfer is still operating. eMMC access is protected by multiple locks (in several subsystems) and the transfer tends to be as asynchronous as possible. Writing on eMMC from&nbsp;<code>panic()</code>&nbsp;would have to terminate those locks and the current transfer and then perform a synchronous write on the medium using a different code-path than the one normally used, which is usually asynchronous.</p>
<p>Despite these constraints, logging the kernel buffer could be implemented using several approaches:</p>
<ol>
<li>Log continuously the kernel log buffer to an external device:
<ul>
<li>Using the network and the&nbsp;<strong>netconsole</strong>&nbsp;driver</li>
<li>Using a serial port and the&nbsp;<strong>console</strong></li>
<li>The log is kept persistent by an external system. On deployed embedded system, this is usually not possible.</li>
</ul>
</li>
<li>Specific driver implementing synchronous write:
<ul>
<li>I found two existing drivers to perform such write:&nbsp;<strong>mtdoops</strong>&nbsp;and&nbsp;<strong>ramoops</strong>. If you are using a MTD or a NVRAM, this may be the easiest solution.</li>
<li>MTD write is perfomed synchronously using&nbsp;<code>mtd_panic_write()</code>. See file&nbsp;<a href="https://github.com/torvalds/linux/blob/master/drivers/mtd/mtdoops.c">mtdoops.c</a></li>
</ul>
</li>
<li>Auxiliary&nbsp;<em>Persistent Storage</em>&nbsp;<strong>Pstore</strong>&nbsp;support:
<ul>
<li>This kernel code allows to use non-volatile, dedicated storage to store debugging information</li>
<li>This is currently limited to&nbsp;<strong>ACPI</strong>. Two LWN articles describe the implementation:&nbsp;<a href="https://lwn.net/Articles/421297/">here</a>&nbsp;and&nbsp;<a href="https://lwn.net/Articles/434821/">here</a></li>
<li>ABRT daemon has a&nbsp;<a href="https://github.com/abrt/abrt/wiki/pstore-oops">support</a>&nbsp;for these dumps</li>
<li>Since version 243, systemd will automatically store any pstore data it finds at boot time to /var/lib/pstore</li>
</ul>
</li>
<li>Execute a new, smaller Linux system on top of the one which crashed using&nbsp;<strong>kexec</strong>
<ul>
<li>Not a well known kernel feature</li>
<li>Some userspace tools available</li>
<li>Quite difficult to implement</li>
</ul>
</li>
</ol>
<p>This article will focus on this 4th solution: using&nbsp;<strong>kexec</strong>&nbsp;feature to boot a new Linux kernel in charge of saving debug information from the initial system. Although the other solutions are easier to use, the latter one is&nbsp;<strong>generic</strong>&nbsp;and does not depends on the physical medium used. This is also often the only solution available for ARM-based systems which do not have MTD to store these dumps.</p>
<h2 id="kexec-and-crashdump-overview">Kexec and crashdump overview</h2>
<p>First of all, we will discuss about <strong>kexec</strong>. This is a feature of the Linux kernel that allows booting into another system (usually another Linux kernel) from a running one. For Desktop systems, this feature is often used to perform fast&nbsp;<em>warm</em>&nbsp;reboots after a kernel update.</p>
<p>Instead of starting a Linux kernel from a bootloader, you are starting it from Linux itself. The idea is to trigger automatically a&nbsp;<em>kexec</em>&nbsp;when a crash occurs. The new booted system would be responsible of storing all debug data on persistent memory. Here is an overview of how&nbsp;<em>kexec</em>&nbsp;can be used for our needs:</p>
<figure class="wp-block-image"><img src="https://www.linuxembedded.fr/wp-content/uploads/2019/11/crashdump.png" alt="" class="wp-image-7104 img-fluid" srcset="https://www.linuxembedded.fr/wp-content/uploads/2019/11/crashdump.png 507w, https://www.linuxembedded.fr/wp-content/uploads/2019/11/crashdump-291x300.png 291w" sizes="(max-width: 507px) 100vw, 507px"></figure>
<p>Triggering&nbsp;<em>kexec</em>&nbsp;from the&nbsp;<code>panic()</code>&nbsp;function is already implemented in the Linux kernel. A dedicated&nbsp;<em>kexec image</em>&nbsp;called&nbsp;<strong>crashdump</strong>&nbsp;can be used to boot this new kernel image when the initial system crashes. To enable it in your kernel build, you need to define the following configuration:</p>
<ul>
<li><code>CONFIG_KEXEC=y</code></li>
<li><code>CONFIG_CRASH_DUMP=y</code></li>
<li><code>CONFIG_PROC_VMCORE=y</code></li>
<li><code>CONFIG_RELOCATABLE=y</code></li>
</ul>
<p>These kernel options not only execute a new kernel on crash but also keep in memory&nbsp;<strong>useful debugging data</strong>&nbsp;which are passed to the new kernel. </p>
<p>What could be the most complete data to perform post-crash investigation ? The answer is simple: the whole volatile memory of the system (RAM). But, if we want to keep it intact while booting a new kernel (which also uses memory), we have to reserve a memory region&nbsp;<strong>from the original kernel</strong>, which means from the boot of the initial system. The picture bellow describes how this initial memory region is dedicated.</p>
<figure class="wp-block-image"><img src="https://www.linuxembedded.fr/wp-content/uploads/2019/11/memory_layout.png" alt="" class="wp-image-7105 img-fluid" srcset="https://www.linuxembedded.fr/wp-content/uploads/2019/11/memory_layout.png 484w, https://www.linuxembedded.fr/wp-content/uploads/2019/11/memory_layout-300x198.png 300w" sizes="(max-width: 484px) 100vw, 484px"></figure>
<p>To tell the initial kernel to reserve this dedicated memory region for crashdump usage, you can use the boot parameter&nbsp;<code>crashkernel=size[KMG][@offset[KMG]]</code>&nbsp;as described in the&nbsp;<a href="https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt">documentation</a>.</p>
<p>When a Linux kernel is booted after a crash, the applications launched by the second kernel are able to access the original memory through the special file&nbsp;<code>/proc/vmcore</code>. Note that this file also contains some&nbsp;<em>metadata</em>&nbsp;to help for debug and forensics.</p>
<p>To sum up, in order to dump the whole memory on a persistent memory, we can use&nbsp;<strong>kexec</strong>&nbsp;feature and define a&nbsp;<strong>crashdump</strong>&nbsp;image which will be booted when a kernel&nbsp;<code>panic()</code>occurs. To prevent the new kernel from overwriting the memory of the crash system, a memory region dedicated to crashdump is reserved at bootime.</p>
<p>Interacting with the&nbsp;<em>kexec</em>&nbsp;kernel part from userspace is done through special&nbsp;<a href="http://man7.org/linux/man-pages/man2/kexec_load.2.html">syscalls</a>&nbsp;which are called by userspace tools provided by the&nbsp;<a href="https://git.kernel.org/pub/scm/utils/kernel/kexec/kexec-tools.git">kexec-tools</a> package. Make sure to use a version compatible with your kernel version.</p>
<p>The&nbsp;<code>kexec</code>&nbsp;utility can be used to load the&nbsp;<em>crashdump</em>&nbsp;kernel in memory and to define its boot parameters. It can also be used to test the kexec feature by booting on-demand to the new kernel. See the&nbsp;<a href="http://man7.org/linux/man-pages/man8/kexec.8.html">man page of kexec</a>&nbsp;for details.</p>
<p>There is also a&nbsp;<code>vmcore-dmesg</code>&nbsp;utility which can be used to extract the kernel log buffer from a&nbsp;<strong>vmcore</strong>. We will see another utility called&nbsp;<code>crash</code>&nbsp;later that can do the same thing.</p>
<h2 id="implementation-of-the-vmcore-backup">Implementation of the vmcore backup</h2>
<p>To understand what is needed to boot a new Linux kernel, you can refer to what your bootloader is doing initially. On embedded system, here is the minimal things a bootloader must do:</p>
<ol>
<li>Load the kernel binary in memory (zImage)</li>
<li>For devicetree-enabled products, load the DTB in memory (myboard.dtb)</li>
<li>Define the&nbsp;<strong>kernel boot parameters</strong>&nbsp;as described&nbsp;<a href="https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt">here</a></li>
<li>Start execution of the new kernel</li>
</ol>
<p>Note that you can define the&nbsp;<strong>root filesystem</strong>&nbsp;of a kernel using&nbsp;<code>root=[device]</code>&nbsp;boot parameter. You can also change the&nbsp;<em>init</em>&nbsp;program executed by the kernel with&nbsp;<code>init=[pgm]</code>&nbsp;parameter if you don&rsquo;t want to execute the default one&nbsp;<code>/sbin/init</code>.</p>
<p>You can choose to use a new kernel binary for crashdump or simply to use the same one. When you have chosen which kernel, devicetree and root partition to use, you can use&nbsp;<code>kexec</code>&nbsp;utility to construct a crashdump image and load it in the dedicated memory:</p>
<pre class="wp-block-code"><code>BOOTARGS="maxcpus=1 reset_devices earlyprintk root=[root partition] init=[your init]"
kexec --type zImage -p [zImage_file] --dtb=[dtb_file] --append="${BOOTARGS}"</code></pre>
<p>Note that you may want to add additional boot parameter depending on your platform. The current boot parameter of a running kernel can be seen in&nbsp;<code>/proc/cmdline</code>.</p>
<p>Then you can simulate a real kernel crash using&nbsp;<em>sysrq</em>&nbsp;(if it is enabled in your kernel):</p>
<pre class="wp-block-code"><code>echo c &gt; /proc/sysrq-trigger</code></pre>
<p>You can either boot on a complete system with a real&nbsp;<code>init</code>&nbsp;like&nbsp;<em>busybox</em>,&nbsp;<em>systemd</em>, <em>SysV</em>&nbsp;or use a minimalist init program which only perform what you want (like in initrd). To test your backup procedure, you can even spawn a shell using&nbsp;<code>init=/bin/sh</code>&nbsp;if there is one in your root partition. Note that there are some limitation in this second system:</p>
<ul>
<li>Memory is limited by the amount of RAM you have reserved using the&nbsp;<code>crashkernel</code>&nbsp;boot parameter of the first Linux. During my tests, I used 64M but it depends on your needs.</li>
<li>You only have one CPU core enabled with boot parameter&nbsp;<code>maxcpus=1</code></li>
<li>Due to small amount of RAM, be carefull not to trigger the OOM Killer !</li>
</ul>
<p>You can do anything needed to backup the&nbsp;<code>vmcore</code>&nbsp;file on your physical persistent storage which can be anything supported by your Linux kernel (eMMC, MTDs, HDD, &hellip;). Here is a sample script to mount a partition and backup the file in it:</p>
<pre class="wp-block-code"><code>mount -t proc proc /proc
mount -t [fstype] /dev/[device] /debug
dd if=/proc/vmcore of=/debug/vmcore bs=1M conv=fsync
umount /debug
sync</code></pre>
<p>Note that&nbsp;<code>conv=fsync</code>&nbsp;prevents from buffering which could lead to OOM triggers as there is not a lot of RAM available.</p>
<h2 id="using-the-vmcore-file">Using the vmcore file</h2>
<p>Once you have saved your&nbsp;<code>vmcore</code>&nbsp;file, you can investigate on what happened in the crashed system and try to find the root cause of your problem.</p>
<p>The easiest-to-use utility I found is&nbsp;<code>crash</code>. See the&nbsp;<a href="https://github.com/crash-utility/crash">github</a>&nbsp;and the&nbsp;<a href="http://people.redhat.com/anderson/crash_whitepaper/">documentation</a>.</p>
<p>Be careful to use a compiled version compatible with your architecture. If you want to build it from source:</p>
<pre class="wp-block-code"><code>git clone https://github.com/crash-utility/crash.git
cd crash
make target=[your target architecture]</code></pre>
<p>In order to use the&nbsp;<code>crash</code>&nbsp;utility, you have to provide the&nbsp;<strong>vmlinux</strong>&nbsp;file corresponding to kernel used&nbsp;<strong>during the crash</strong>&nbsp;(the one of the nominal system). Generally, embedded systems use&nbsp;<code>zImage</code>&nbsp;format, so you will also need to keep the&nbsp;<code>vmlinux</code>&nbsp;version of that kernel at compilation time.</p>
<p>Then, to use&nbsp;<code>crash</code>, just launch it with your&nbsp;<code>vmlinux</code>&nbsp;and your&nbsp;<code>vmcore</code>:</p>
<pre class="wp-block-code"><code>$ ~/tools/crash/crash vmlinux vmcore
      KERNEL: vmlinux
    DUMPFILE: vmcore
        CPUS: 2 [OFFLINE: 1]
       PANIC: "sysrq: SysRq: Trigger a crash"</code></pre>
<p>You will get a lot of useful information. Here is a list of command you can use to do offline debugging:</p>
<ul>
<li><code>log</code>: extract the kernel log buffer</li>
<li><code>bt</code>: show the backtrace</li>
<li><code>rd [addr]</code>: read memory at the given address</li>
<li><code>ps</code>: extract the process list when the crash occurs</li>
</ul>
<p>You can also use the&nbsp;<code>help</code>&nbsp;command for complete list:</p>
<pre class="wp-block-code"><code>    crash&gt; help

    *              extend         log            rd             task           
    alias          files          mach           repeat         timer          
    ascii          foreach        mod            runq           tree           
    bpf            fuser          mount          search         union          
    bt             gdb            net            set            vm             
    btop           help           p              sig            vtop           
    dev            ipcs           ps             struct         waitq          
    dis            irq            pte            swap           whatis         
    eval           kmem           ptob           sym            wr             
    exit           list           ptov           sys            q     </code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>This article has presented one solution to backup crashed product memory before rebooting. This can be useful for unstable products which are already deployed. Among all listed solutions, the&nbsp;<strong>kexec</strong>&nbsp;one is hardware-agnostic and should be usable with not-too-old kernels on various architecture (tested on ARMv7).</p>
<p>However there are two impacts on&nbsp;<em>runtime</em>&nbsp;when loading a crashdump image:</p>
<ol>
<li>You have to reserve a small amount of RAM so there is less for the nominal system</li>
<li>Rebooting after a crash may take some time if you write lots of data in slow persistent storage. Adding to the downtime of the product.</li>
</ol>
<p>The&nbsp;<strong>crashdump</strong>&nbsp;image of&nbsp;<strong>kexec</strong>&nbsp;is meant to boot a new system when the first one crashes. There are a lot of possible usecases using this feature, not only to backup debugging data.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/kernel_administration_guide/kernel_crash_dump_guide">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/kernel_administration_guide/kernel_crash_dump_guide</a></li>
<li><a href="https://events.static.linuxfound.org/slides/2011/linuxcon-japan/lcj2011_wang.pdf">https://events.static.linuxfound.org/slides/2011/linuxcon-japan/lcj2011_wang.pdf</a></li>
<li><a href="http://lse.sourceforge.net/kdump/documentation/ols2oo5-kdump-paper.pdf">http://lse.sourceforge.net/kdump/documentation/ols2oo5-kdump-paper.pdf</a></li>
<li><a href="https://wiki.archlinux.org/index.php/kexec">https://wiki.archlinux.org/index.php/kexec</a></li>
<li><a href="https://help.ubuntu.com/lts/serverguide/kernel-crash-dump.html">https://help.ubuntu.com/lts/serverguide/kernel-crash-dump.html</a></li>
</ul>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>FlameGraph</title>
		<link>https://dev-pro.xyz/blog/flamegraph/</link>
				<pubDate>Fri, 27 Dec 2019 15:50:34 +0000</pubDate>
		<dc:creator><![CDATA[siz]]></dc:creator>
				<category><![CDATA[debug]]></category>
		<category><![CDATA[kernel]]></category>
		<category><![CDATA[Linux]]></category>
		<category><![CDATA[perf]]></category>
		<category><![CDATA[Technologie]]></category>

		<guid isPermaLink="false">https://dev-pro.xyz/blog/flamegraph/</guid>
				<description><![CDATA[Introduction Les outils de profilage permettent lors de l&#8217;ex&#233;cution d&#8217;un logiciel de contr&#244;ler la liste des fonctions appel&#233;es, le temps pass&#233; dans chacune d&#8217;elle, l&#8217;utilisation des ressources processeur ou l&#8217;utilisation m&#233;moire par exemple. Sous Linux une multitude d&#8217;outils sont disponibles et si vous avez d&#233;j&#224; utilis&#233; Perf ou eBPF vous avez sans nul doute remarqu&#233; ... <a href="https://dev-pro.xyz/blog/flamegraph/" class="more-link text-uppercase small"><strong>Continue Reading</strong> <i class="fa fa-angle-double-right" aria-hidden="true"></i></a>]]></description>
								<content:encoded><![CDATA[<div>
<h1>Introduction</h1>
<p> Les outils de profilage permettent lors de l&rsquo;ex&eacute;cution d&rsquo;un logiciel de contr&ocirc;ler la liste des <a href="https://fr.wikipedia.org/wiki/Fonction_informatique">fonctions</a> appel&eacute;es, le temps pass&eacute; dans chacune d&rsquo;elle, l&rsquo;utilisation des ressources <a href="https://fr.wikipedia.org/wiki/Processeur">processeur</a> ou l&rsquo;utilisation <a href="https://fr.wikipedia.org/wiki/M%C3%A9moire_(informatique)">m&eacute;moire</a> par exemple. Sous Linux une multitude d&rsquo;outils sont disponibles et si vous avez d&eacute;j&agrave; utilis&eacute; Perf ou eBPF vous avez sans nul doute remarqu&eacute; que la quantit&eacute; de log g&eacute;n&eacute;r&eacute;e peut rapidement devenir gargantuesque et donc difficilement interpr&eacute;table.</p>
<p>Cet article va vous pr&eacute;senter les FlameGraph : un outil tr&egrave;s pratique de visualisation des logs d&rsquo;applications profil&eacute;es qui a &eacute;t&eacute; d&eacute;velopp&eacute; par Brendan Gregg, ing&eacute;nieur chez Netflix et sp&eacute;cialiste de l&rsquo;analyse de performance. Les FlameGraph sont une une repr&eacute;sentation des logs de n&rsquo;importe quel outil de g&eacute;n&eacute;ration de donn&eacute;es de profiling comme eBPF et Perf qui sont &eacute;galement des traceurs d&eacute;j&agrave; introduits par les excellents articles de Jugurtha :</p>
<ul>
<li><a href="http://www.linuxembedded.fr/2018/12/les-traceurs-sous-linux-1/">http://www.linuxembedded.fr/2018/12/les-traceurs-sous-linux-1/</a>&nbsp;: introduction au tra&ccedil;age et profilage d&rsquo;applications ainsi que le principe de fonctionnement de Ftrace et ses outils front-end.</li>
<li><a href="http://www.linuxembedded.fr/2019/02/les-traceurs-sous-linux-2/">http://www.linuxembedded.fr/2019/02/les-traceurs-sous-linux-2/</a>&nbsp;: utilisation de perf, avec des exemples de commandes utiles que je vais utiliser dans cet article.</li>
<li><a href="http://www.linuxembedded.fr/2019/03/les-secrets-du-traceur-ebpf/">http://www.linuxembedded.fr/2019/03/les-secrets-du-traceur-ebpf/</a></li>
</ul>
<p>Cet article n&rsquo;est qu&rsquo;un exemple d&rsquo;utilisation des FlameGraph pr&eacute;c&eacute;d&eacute; de quelques notions. Tout le m&eacute;rite revient &eacute;videmment &agrave; Brendan Gregg. Vous pouvez retrouver son blog qui sert de r&eacute;f&eacute;rence aux m&eacute;thodes de profilage, au lien suivant : <a href="http://www.brendangregg.com/overview.html">http://www.brendangregg.com/overview.html</a></p>
<h2>G&eacute;n&eacute;ration d&rsquo;un FlameGraph on-CPU</h2>
<p>Une des mani&egrave;res de profiler une application revient &agrave; d&eacute;terminer pourquoi le CPU est occup&eacute;. Une fa&ccedil;on efficace de faire cela est le profilage par &eacute;chantillonnage&nbsp;: on envoie &agrave; une certaine fr&eacute;quence une interruption au CPU pour r&eacute;cup&eacute;rer la stack trace, l&rsquo;adresse en m&eacute;moire de l&rsquo;instruction en cours d&rsquo;ex&eacute;cution (Program Counter) ainsi que l&rsquo;adresse de la fonction. Nous allons dans notre exemple utiliser la commande Perf pour ce faire.</p>
<p>Vous pouvez installer Perf sur votre distribution via votre gestionnaire de paquet&nbsp;: linux-perf sous debian, perf sous CentOS et Arch, linux-tools sous Ubuntu&hellip; De plus si vous voulez qu&rsquo;un utilisateur non root puisse collecter des donn&eacute;es dans votre terminal courant, il est possible de modifier la valeur de la variable perf_event_paranoid&nbsp;:</p>
<pre class="wp-block-preformatted"> <strong>echo -1 &gt; /proc/sys/kernel/perf_event_paranoid</strong>.</pre>
<p>NB : les programmes que vous profilez doivent comporter des symboles de debug n&eacute;cessaires &agrave; la traduction des adresses m&eacute;moire en nom de fonction.</p>
<p>Si vous voulez profiler une application int&eacute;gr&eacute;e dans votre distribution via Yocto il faut installer la version  &laquo;&nbsp;-dbg&nbsp;&raquo; du paquet que vous souhaitez analyser. Vous pouvez &eacute;galement utiliser l&rsquo;image feature &laquo;&nbsp;dbg-pkgs&nbsp;&raquo; pour cr&eacute;er une version de votre image int&eacute;grant tous les paquets de debug ce qui peut &ecirc;tre utile pour profiler le syst&egrave;me complet.</p>
<p>Sur Debian pour installer des paquets avec les symboles de debug il faut ajouter la source <strong>deb</strong> <strong>http://debug.mirrors.debian.org/debian-debug/ buster-debug main</strong> (pour debian buster) dans votre source.list d&rsquo;apt. Apr&egrave;s &ccedil;a vous pouvez installer les paquets de debug qui ont en g&eacute;n&eacute;ral comme suffixe -dbgsym.</p>
<p>Une autre source potentielle de probl&egrave;me peut &ecirc;tre que la stack trace retourn&eacute;e est incompl&egrave;te pour les applications qui sont compil&eacute;es avec des optimisations de compilation. Dans ce cas il faut recompiler l&rsquo;application avec l&rsquo;option <strong>&ndash;</strong><strong>fno-omit-frame-pointer.</strong></p>
<p>De la m&ecirc;me mani&egrave;re il est possible que la stack trace du kernel soit incompl&egrave;te si l&rsquo;option CONFIG_FRAME_POINTER est d&eacute;sactiv&eacute;e (Kernel hacking/Compile-time-checks and compiler option)</p>
<p>La proc&eacute;dure pour g&eacute;n&eacute;rer les FlameGraph CPU est tr&egrave;s simple, il suffit dans un premier temps de lancer la commande suivante pour profiler pendant 30 secondes et &agrave; une fr&eacute;quence de 99Hz (99 interruptions par seconde) une application qui a un PID valant 12345 par exemple&nbsp;:</p>
<pre class="wp-block-preformatted"><strong>perf record -F 99 -p 12345 -g -- sleep 30</strong></pre>
<p>On peut &eacute;galement profiler le syst&egrave;me complet et donc tous les coeurs de la CPU avec l&rsquo;option -a&nbsp;:</p>
<pre class="wp-block-preformatted"><strong>perf record -F 99 -a -g -- sleep 30</strong></pre>
<p>Cela va g&eacute;n&eacute;rer un fichier perf.data qui contient les &eacute;chantillons qui peuvent &ecirc;tre lus via la commande perf report :</p>
<pre class="wp-block-preformatted"><strong>perf report -n --stdio</strong> </pre>
<p>Perf est un outil tr&egrave;s puissant mais en lan&ccedil;ant les 2 derni&egrave;res commandes sur ma machine, le rapport g&eacute;n&eacute;r&eacute; fait plus d&rsquo;un millier de lignes. Et c&rsquo;est bien l&agrave; l&rsquo;int&eacute;r&ecirc;t des FlameGraph. Ils sont tr&egrave;s facile &agrave; g&eacute;n&eacute;rer et tr&egrave;s faciles &agrave; interpr&eacute;ter rapidement.</p>
<p>Dans mon cas j&rsquo;ai g&eacute;n&eacute;r&eacute; mes Flame Graphs sur une cible dont la distribution a &eacute;t&eacute; g&eacute;n&eacute;r&eacute;e via Yocto ; voici une recette tr&egrave;s simple qui va r&eacute;cup&eacute;rer les sources du projet et les installer dans la cible&nbsp;: </p>
<pre class="wp-block-preformatted">SUMMARY = "Flamegrah Yocto recipe"
DESCRIPTION = "Flamegraph are a visualization tool for profiled application logs"
LICENSE = "CLOSED"

S = "${WORKDIR}/git"

SRC_URI = "git://github.com/brendangregg/FlameGraph.git;protocol=https"
SRCREV = "1b1c6deede9c33c5134c920bdb7a44cc5528e9a7"

RDEPENDS_flamegraph = "perl"

FILES_${PN} += "flamegraph/"
do_install() {
     install -d  ${D}/flamegraph
     install -m  0755 ${S}/*.pl ${D}/flamegraph
}</pre>
<p>Vu que rien n&rsquo;est compil&eacute; dans le projet vous pouvez &eacute;galement cloner le projet &agrave; l&rsquo;URI suivant et le copier sur la cible&nbsp;: </p>
<pre class="wp-block-preformatted"><strong>git clone https://github.com/brendangregg/FlameGraph</strong></pre>
<p>Le projet consiste en une multitude de scripts perl ainsi que des exemples de FlameGraph d&eacute;j&agrave; g&eacute;n&eacute;r&eacute;s et pr&eacute;sent&eacute;s sur le blog de Brendan Gregg. A partir d&rsquo;un fichier perf.data g&eacute;n&eacute;r&eacute; par <em>perf</em> on peut le copier dans le r&eacute;pertoire du projet et lancer la commande suivante pour g&eacute;n&eacute;rer un flamegraph :</p>
<pre class="wp-block-preformatted"><strong>perf script | ./stackcollapse-perf.pl &gt; out.perf-folded &amp;&amp; ./flamegraph.pl out.perf-folded &gt; flamegraph.svg</strong> </pre>
<ul>
<li>perf script va chercher dans le r&eacute;pertoire local un fichier perf.data (g&eacute;n&eacute;r&eacute; par perf record) et afficher la trace. Attention n&eacute;anmoins cette commande est d&eacute;pendante de l&rsquo;architecture de la plateforme. Il faut donc g&eacute;n&eacute;rer les Flamegraph directement sur la cible.</li>
<li>stackcollapse-perf va formater la trace en une seule ligne pour qu&rsquo;elle puisse &ecirc;tre trait&eacute;e par le script flamegraph.pl</li>
<li>flamegraph.pl transforme le fichier out.perf-folded en une image de flamegraph.</li>
</ul>
<p>On g&eacute;n&egrave;re donc un fichier SVG qui peut facilement &ecirc;tre ouvert depuis un navigateur. Je vais vous pr&eacute;senter ici un exemple tr&egrave;s simple de FlameGraph issu d&rsquo;un petit programme.</p>
<p>D&rsquo;une part le programme va dans un premier thread ouvrir un fichier, &eacute;crire dedans, le refermer en boucle et va dans un autre thread lancer une boucle vide qui va faire mouliner le processeur. Si on profile le syst&egrave;me entier pendant 60 secondes et qu&rsquo;on lance pendant cette p&eacute;riode le programme pendant 30 secondes on obtient le r&eacute;sultat suivant&nbsp;:</p>
<figure class="wp-block-image is-resized"><img src="http://www.linuxembedded.fr/wp-content/uploads/2019/09/flamegraph.svg_-1024x633.png" alt="" class="wp-image-6947 img-fluid" width="827" height="511" srcset="https://www.linuxembedded.fr/wp-content/uploads/2019/09/flamegraph.svg_-1024x633.png 1024w, https://www.linuxembedded.fr/wp-content/uploads/2019/09/flamegraph.svg_-300x186.png 300w, https://www.linuxembedded.fr/wp-content/uploads/2019/09/flamegraph.svg_-768x475.png 768w, https://www.linuxembedded.fr/wp-content/uploads/2019/09/flamegraph.svg_.png 1250w" sizes="(max-width: 827px) 100vw, 827px"></figure>
<p>Interpr&eacute;tation du r&eacute;sultat&nbsp;:</p>
<ul>
<li>Chaque boite repr&eacute;sente l&rsquo;appel &agrave; une fonction dans la pile</li>
<li>L&rsquo;axe des ordonn&eacute;es pr&eacute;sente la profondeur de la pile</li>
<li>La largeur des frame en abscisse correspond au temps pass&eacute; (nombre d&rsquo;&eacute;chantillons) par un CPU &agrave; ex&eacute;cuter la fonction correspondante.</li>
</ul>
<p>Dans notre cas il est tr&egrave;s facile de d&eacute;celer les t&acirc;ches gourmandes en CPU. Pour interpr&eacute;ter un Flame Graph on va chercher les boites larges tout en haut de la pile et voir par quelles fonctions elles ont &eacute;t&eacute; appel&eacute;es. Ici on remarque que la case &laquo;&nbsp;cpu_moulineur&nbsp;&raquo; est extr&ecirc;mement large, c&rsquo;est elle qui correspond &agrave; la fonction contenant une boucle vide.</p>
<p>Si on positionne le curseur de la souris sur la boite &laquo;&nbsp;cpu_moulineur&nbsp;&raquo;&nbsp;un champ affiche le nombre d&rsquo;&eacute;chantillons (et donc le temps pass&eacute; dans la fonction) ainsi que le pourcentage correspondant par rapport &agrave; la mesure compl&egrave;te.</p>
<p>Le bloc situ&eacute; &agrave; sa droite correspond &agrave; la fonction qui ouvre et ferme un fichier en boucle. En plus d&rsquo;observer le nombre d&rsquo;&eacute;chantillons pour chaque boite, il est int&eacute;ressant de voir l&rsquo;encha&icirc;nement des fonctions appel&eacute;es de l&rsquo;userspace jusqu&rsquo;aux strates les plus enfouies du kernel.</p>
<p>Ici l&rsquo;exemple est relativement trivial mais dans un plus gros projet cela peut s&rsquo;av&eacute;rer tr&egrave;s utile car les FlameGraph offrent une vision globale des fonctions appel&eacute;es par une application&nbsp;! On peut voir par exemple si l&rsquo;appel &agrave; une fonction userspace provoque l&rsquo;appel &agrave; un kmalloc c&ocirc;t&eacute; kernel.</p>
<p>NB&nbsp;: on ne sait n&eacute;anmoins pas &agrave; quel moment les fonctions sont appel&eacute;es car il n&rsquo;y a pas de notion temporelle dans les flamegraph CPU.</p>
<h1>D&rsquo;autres types de FlameGraph int&eacute;ressants</h1>
<h2>FlameGraph off-CPU</h2>
<p>Les FlameGraph on-CPU permettent de comprendre l&rsquo;usage CPU mais ne permettent pas de voir les probl&egrave;mes de latence pr&eacute;sents quand un thread est en attente d&rsquo;une I/O bloqu&eacute;e, d&rsquo;un timer ou quand il y a un changement de contexte. Cela constitue une forme d&rsquo;analyse &agrave; part enti&egrave;re que Brendan Gregg appelle analyse off-CPU (en opposition &agrave; on-CPU).</p>
<p>En r&eacute;sum&eacute; l&rsquo;analyse off-CPU est un moyen de localiser de la latence introduite par le blocage de thread, cette analyse est compl&eacute;mentaire &agrave; l&rsquo;analyse on-CPU et est n&eacute;cessaire &agrave; la compr&eacute;hension du cycle de vie d&rsquo;un thread.</p>
<p>Leur g&eacute;n&eacute;ration peut &ecirc;tre r&eacute;alis&eacute;e via le script offcputime de bcc (je vous renvoie vers l&rsquo;article de Jugurtha) qui permet de trouver pourquoi et pendant combien de temps un thread est bloqu&eacute; et ce quelque soit le type de blocage.</p>
<p>Une approche est de tracer les appels aux fonctions malloc et free et afficher sur un Flame Graph le nombre de fois o&ugrave; les fonctions ont a &eacute;t&eacute; appel&eacute;es ou le nombre de bytes qui ont &eacute;t&eacute; allou&eacute;s pour chaque frame.</p>
<h1>Conclusion</h1>
<p>En r&eacute;sum&eacute; le Flame Graph est un puissant outil de visualisation de logs d&rsquo;outils d&rsquo;analyse de performance qui peut vous permettre de gagner un temps pr&eacute;cieux. J&rsquo;ai simplement voulu vous partager cette d&eacute;couverte dans cet article qui n&rsquo;est qu&rsquo;une rapide pr&eacute;sentation, je vous invite une nouvelle fois &agrave; vous rendre sur le blog de Brendan Gregg pour beaucoup plus de d&eacute;tails !</p>
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
